{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zy0Y6xTLNtM6"
   },
   "source": [
    "# Introductie tot Deep Learning met Keras en TensorFlow\n",
    "## Les 9. Neurale netwerken in de praktijk\n",
    "\n",
    "**Originele versie: Daniel Moser (UT Southwestern Medical Center)**\n",
    "\n",
    "**Aangepast door Tijmen Muller en Joost Vanstreels (2021)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cro1QI9NtM7"
   },
   "source": [
    "Tijdens de eerste les van de cursus Machine Learning zijn we aan de slag gegaan met neurale netwerken en hebben we een korte introductie gegeven van convolutionele neurale netwerken (CNN's of ConvNets), één van de verschillende deep learning algoritmen.\n",
    "\n",
    "In les 10 gaan we echt aan de slag met ConvNets, in deze les gaan we nog even goed kijken naar neurale netwerken. Sinds les 1 hebben jullie een heleboel geleerd over neurale netwerken en de code in dit notebook zou nu zo klaar als een klontje moeten zijn.\n",
    "\n",
    "In dit notebook gaan we een aantal onderdelen van het vorige notebook nader toelichten en naar een aantal alternatieve oplossingen kijken. Het doel is om meer praktijkgerichte ervaring te krijgen met neurale netwerken, TensorFlow en Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mo8rdjbBNtM7"
   },
   "source": [
    "<img src=\"https://i.imgur.com/86wacJG.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OjDwn6ArNtM7"
   },
   "source": [
    "## Benodigde libraries \n",
    "\n",
    "We hebben een aantal libraries nodig, sommigen kennen jullie al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QfscTgLkNtM8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.datasets import mnist     # MNIST dataset is onderdeel van Keras\n",
    "from tensorflow.keras.models import Sequential  # Het type neuraal netwerk dat we gaan gebruiken\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation  # Verschillende type lagen die we gaan gebruiken\n",
    "from tensorflow.keras.optimizers import SGD, Adam, schedules\n",
    "from tensorflow.keras import utils                              # NumPy gerelateerde tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QljqpqwxNtM8"
   },
   "source": [
    "## Stap 1. Data exploration \n",
    "\n",
    "De MNIST dataset is gebundeld in Keras, we kunnen deze eenvoudig downloaden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRVKdbbDNtM8"
   },
   "outputs": [],
   "source": [
    "# Inladen van de dataset, deze is al gesplitst in een train- en testset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(\"x_train shape\", x_train.shape)\n",
    "print(\"y_train shape\", y_train.shape)\n",
    "print(\"x_test shape\", x_test.shape)\n",
    "print(\"y_test shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_qbg_vsNtM9"
   },
   "source": [
    "Met matplotlib kunnen we een aantal plaatjes uit de dataset bekijken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5E9Y2SMNtM-"
   },
   "source": [
    "## Stap 2. Data preparation\n",
    "### Features\n",
    "\n",
    "We hebben vorige keer al gezien dat we de 28x28 plaatjes moeten *flatten* naar een vector met een lengte van 748. Hieronder zie je een hoe dat werkt voor een klein deel van een plaatje:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmvZVFR2NtM-"
   },
   "source": [
    "<img src='https://i.imgur.com/l049B93.png' >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8PWzi6ilNtM-"
   },
   "outputs": [],
   "source": [
    "# Let op: we schakelen hier over van een kleine letter naar een hoofdletter, zodat de originele\n",
    "# Trainingsdata in `x_train` beschikbaar blijft!\n",
    "\n",
    "X_train = x_train.reshape(60_000, 784)  # Reshape de 60.000 plaatjes van 28 x 28 matrices naar 60.000 784-lengte vectoren.\n",
    "X_test = x_test.reshape(10_000, 784)\n",
    "\n",
    "X_train = X_train.astype(np.float32)   # Verander datatype van integers naar 32-bit floats\n",
    "X_test = X_test.astype(np.float32) \n",
    "\n",
    "X_train /= 255                         # Normaliseer de pixels door de waarde te delen door de maximale waarde (= 255)\n",
    "X_test /= 255\n",
    "\n",
    "print(\"Training matrix shape:\", X_train.shape)\n",
    "print(\"Testing matrix shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBfvAPyKNtM-"
   },
   "source": [
    "### Target\n",
    "\n",
    "De target variabele is een 0, 1, 2, ... of 9. Dat zijn verschillende klasses of categorieën. Kijk bijvoorbeeld maar eens naar de inhoud van een willekeurig item uit y_train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rD1M2Ehy4t4k"
   },
   "outputs": [],
   "source": [
    "i = random.randrange(X_train.shape[0])\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(X_train[i].reshape(28, 28), cmap=\"gray\", interpolation=\"none\")\n",
    "plt.title(f\"Dit is een: {y_train[i]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qbgh99vpNtM_"
   },
   "outputs": [],
   "source": [
    "# Met tensorflow.keras.utils wordt de one-hot encoding gedaan\n",
    "nb_classes = 10 # Aantal classes\n",
    "\n",
    "Y_train = utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "print(f\"Y_train: {Y_train.shape}, Y_test: {Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2z7yyUUpNtM_"
   },
   "source": [
    "## Stap 3. Modelling\n",
    "We hebben de vorige keer een fully connected 3-layer network gemaakt:\n",
    "\n",
    "<img src=\"https://i.imgur.com/1MR9U5c.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LIqyfoI4t4l"
   },
   "source": [
    "### Model opbouwen\n",
    "Dat model hebben we als volgt opgebouwd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCHkvLi74t4t"
   },
   "source": [
    "#### De eerste laag\n",
    "\n",
    "In ons eerste model hadden we een input van 784 en output van 10. Nu is de input weer 784 maar de output is een eerste **hidden layer** van 512 elementen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RslIFi8O4t4u"
   },
   "outputs": [],
   "source": [
    "# Gebruik model.add om de laag toe te voegen aan het model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,))) # (784,) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MbeSEt1d4t4u"
   },
   "outputs": [],
   "source": [
    "# Daarna voeg je de activatiefunctie 'sigmoid' weer toe\n",
    "model.add(Activation(\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9mdtE7U4t4n"
   },
   "source": [
    "#### **NIEUW**: Dropout\n",
    "Neurale netwerken kunnen soms overfitten. Overfitten kan als gevolg hebben dat een neuron z’n input krijgt van slechts één of een klein aantal neuronen van de vorige laag. Deze afhankelijkheid kan ervoor zorgen dat het netwerk goed werkt voor bepaalde trainingsvoorbeelden, maar niet op testgevallen met als gevolg overfitting.\n",
    "\n",
    "Het doel van dropout is om deze afhankelijkheid te voorkomen. Dropout-nodes zorgen ervoor dat tijdens het trainen willekeurige neuronen uitgezet worden en later weer aangezet worden. Het gevolg is dat het een output-neuron z’n input ook van andere neuronen moet halen. Het gevolg is dat het netwerk robuuster wordt.\n",
    "\n",
    "De dropout-lagen worden alleen gebruikt tijdens het trainen. Tijdens het evalureren van je model worden deze lagen uitgeschakelt door TensorFlow.\n",
    "\n",
    "Meer info vind je in het boek van [Nielsen](http://neuralnetworksanddeeplearning.com/chap3.html#other_techniques_for_regularization).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Opdracht** \n",
    "\n",
    "Voeg de dropout laag toe met waarde 0.2, zodat 20% van de inputs van de volgende laag wordt uitgeschakeld. Je kunt hiervoor `Dropout` gebruiken (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npICIBEL4t4v",
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tclj9vPINtNC"
   },
   "source": [
    "#### De tweede laag\n",
    "\n",
    "Bij de hieropvolgende lagen hoef je geen input te definiëren: de input is de vorige laag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwuGgKhPNtNC"
   },
   "outputs": [],
   "source": [
    "# Voeg hier nog een laag van 512 neuronen toe met een sigmoid activation een dropout van 0.2\n",
    "model.add(Dense(512))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSDltoq3NtNC"
   },
   "source": [
    "####  De output laag\n",
    "\n",
    "De laatste laag heeft 10 elementen: de 10 klasses. \n",
    "\n",
    "#### **NIEUW**: Softmax\n",
    "Het doel van de neurale netwerken die we gezien hebben is classificeren. Op dit moment is de output een laag met outputneuronen en voor elke neuron een waarde. Elke afzonderlijke waarde zegt niet zoveel, het is belangrijk om ze onderling te vergelijken. Hiermee kun je twee zaken bepalen:\n",
    "\n",
    "- Wat is de voorspelling? Dat is de neuron met de hoogste waarde.\n",
    "- Hoe zeker is het netwerk van zijn voorspelling? Hoe hoog is deze hoogste waarde in vergelijking met de andere waarden.\n",
    "\n",
    "Door de softmax functie te gebruiken, worden de waardes van de outputneuronen aangepast zodanig dat de som = 1 en dat de waardes eigenlijk zeggen: '*met X% zekerheid is de voorspelling y*'.\n",
    "\n",
    "Meer info vind je in het boek van [Nielsen](http://neuralnetworksanddeeplearning.com/chap3.html#softmax).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Opdracht**\n",
    "\n",
    "Voeg een laag van 10 neuronen toe met een activation 'softmax', een dropout is niet nodig. Je moet hiervoor `Activation` gebruiken (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Activation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2rjYaltNtND",
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhVRDLgO4t4o"
   },
   "source": [
    "Nu hebben we ons model helemaal gebouwd. Het eindresultaat zou er als volgt uit moeten zien:\n",
    "\n",
    "```\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense (Dense)                (None, 512)               401920    \n",
    "_________________________________________________________________\n",
    "activation (Activation)      (None, 512)               0         \n",
    "_________________________________________________________________\n",
    "dropout (Dropout)            (None, 512)               0         \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 512)               262656    \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 512)               0         \n",
    "_________________________________________________________________\n",
    "dropout_1 (Dropout)          (None, 512)               0         \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 10)                5130      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 10)                0         \n",
    "=================================================================\n",
    "Total params: 669,706\n",
    "Trainable params: 669,706\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RUx-anMmNtND"
   },
   "outputs": [],
   "source": [
    "# Laten we even kijken hoe dat eruit ziet\n",
    "model.summary()\n",
    "\n",
    "# We slaan de parameters van het model op, zodat je ze eventueel later kunt gebruiken\n",
    "weights_init = model.get_weights()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZG6N2xS4t4w"
   },
   "source": [
    "### Compilen\n",
    "\n",
    "Het vastleggen van de hyperparameters van een model doe je bij het *compilen*. Je herkent in de code hieronder ongetwijfeld de hyperparameters learning rate en *SGD* (die staat voor de *stochastic gradient descent* optimizer). Bij de SGD is een extra parameter toegevoegd: *momentum*. \n",
    "\n",
    "De standaard SGD-optimizer heeft als nadeel dat er één stapgrootte wordt genomen voor alle dimensies, ondanks dat de gradient van dimensies verschillend kan zijn. Het momentum algoritme zorgt voor een unieke stapgrootte voor elke dimensie. In onderstaande code is voor momentum een parameter toegevoegd. Meer informatie over momentum vind je in het boek van [Nielsen](http://neuralnetworksanddeeplearning.com/chap3.html#variations_on_stochastic_gradient_descent).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD met momentum\n",
    "opt = SGD(learning_rate=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **NIEUW**: Cross entropy loss function\n",
    "De standaard Mean Squared Error loss function heeft een aantal nadelen die het convergeren moeilijker maken. Bij MNIST werkt de MSE function prima, maar bij complexere problemen niet meer. De cross entropy loss function, die ook gebruikt wordt bij decision trees, lost een aantal van die problemen op en wordt in de praktijk veel vaker gebruikt. \n",
    "\n",
    "Meer info vind je in het boek van [Nielsen](http://neuralnetworksanddeeplearning.com/chap3.html#the_cross-entropy_cost_function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Opdracht**\n",
    "\n",
    "Compileer het model met `compile()`, waarbij je gebruik maakt van de cross entropy loss function. Zie ook https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CAcJ8UmP4t4w",
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U72jBYYo4t4q"
   },
   "source": [
    "### Het model trainen\n",
    "\n",
    "We hebben gezien dat je bij neurale netwerken niet altijd de optimale oplossing kunt vinden. Daarom ga je op zoek naar een oplossing die goed genoeg is. Of naar de beste oplossing in een bepaalde tijd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EAwHn8YNtNG"
   },
   "source": [
    "Een manier om te spelen met het trainen, is de keuze voor een **batch size**. Dit bepaalt hoeveel items uit de dataset per stap bekeken worden, voordat de gewichten geüpdate worden. Grotere batch sizes zorgt voor sneller trainen, maar hebben als risico dat je in een lokaal minimum blijft hangen. Het zoeken naar de beste batch size is vaak een kwestie van trial en error.\n",
    "\n",
    "Daarnaast is het aantal **epochs** ook belangrijk. Dit geeft aan hoe vaak je de hele trainset wilt gebruiken. Het is niet zo dat je na het bekijken van alle items uit de trainset klaar bent, je kunt de trainset nog een keer gebruiken om het model te verbeteren. En nog een keer. Enzovoorts. Zie het als het studeren voor een tentamen waarbij je een oefententamen vaker maakte om de stof te begrijpen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uza088eBNtNH"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          verbose=1) # Verbose zorgt voor een kekke animatie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oj3-sTMGNtNH"
   },
   "source": [
    "Je ziet de **loss** en **accuracy**. De accuracy kennen jullie: dat is het percentage goed voorspelde cijfers. De loss functie is een functie die het verschil tussen de voorspellingen en testset berekent. Deze loss functie is erg belangrijk want deze wordt gebruikt om te trainen. De optimalisatiefunctie zoekt naar manieren om de loss functie te verlagen. Een loss van 0 is het ultieme doel!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JT4Cx2u7NtNH"
   },
   "source": [
    "## Stap 4. Evalueren\n",
    "\n",
    "Leuk, die accuracy op de traindata, maar hoe goed doet ie het op de testdata?! We kijken weer naar de accuracy maar ook naar de loss (alhoewel die minder interessant is op dit moment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8entMBoNtNH"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, Y_test)\n",
    "print(f\"Test loss: {loss:.4f}\")\n",
    "print(f\"Test accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGUy0tt6NtNI"
   },
   "source": [
    "Als het goed is bereik je hiermee een accuracy van ruim 95%. Dat is niet slecht, maar met een aantal simpele aanpassingen kunnen we de accuracy makkelijk verhogen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56Wqm3eaEJOz"
   },
   "source": [
    "# Deel II. Nieuwe mogelijkheden van neurale netwerken\n",
    "De volgende opdrachten staan in het teken van een aantal nieuwe mogelijkheden van neurale netwerken die in de praktijk veel gebruikt worden.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuPOq08fGPta"
   },
   "source": [
    "## Opdracht 1. Adam optimizer in plaats van SGD\n",
    "\n",
    "In de praktijk is gebleken dat het *standaard* SGD algoritme de nodige beperkingen heeft qua performance en resultaten, ook mét het toevoegen van *momentum*. De [RMSProp](https://www.youtube.com/watch?v=_e-LFe_igno&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc) optimizer haalde al betere resultaten maar de meest gebruikte optimizer op dit moment is de [Adam](https://www.youtube.com/watch?v=JXQT_vxqwIs&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc) optimizer die SGD, momentum en RMSProp combineert. \n",
    "\n",
    "De links verwijzen naar de video's van [deeplearning.ai](https://deeplearning.ai).\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvEOwXtIGPtb"
   },
   "source": [
    "### Opdracht\n",
    "Experimenteer met de Adam optimizer. Vergelijk de resultaten met het SGD en momentum. Kijk naar de snelheid van trainen en de score van het model.\n",
    "\n",
    "**Tip**: gooi eventueel alle code hieronder in één cel zodat je snel kunt experimenteren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bVo_mssGPtb"
   },
   "outputs": [],
   "source": [
    "model_adam = Sequential()\n",
    "model_adam.add(Dense(512, input_shape=(784,))) # (784,)\n",
    "model_adam.add(Activation(\"sigmoid\"))\n",
    "model_adam.add(Dropout(0.2))\n",
    "\n",
    "model_adam.add(Dense(512))\n",
    "model_adam.add(Activation(\"sigmoid\"))\n",
    "model_adam.add(Dropout(0.2))\n",
    "\n",
    "model_adam.add(Dense(10))\n",
    "model_adam.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vul in de cel hieronder de juiste code in om de Adam optimizer te gebruiken (zie https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) en om het model te compileren met `compile()`. Denk ook aan de loss functie en eventuele metrieken (`metrics`) die je wil gebruiken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adam.fit(X_train, Y_train,\n",
    "               batch_size=128,\n",
    "               epochs=5,\n",
    "               verbose=1) \n",
    "\n",
    "loss, accuracy = model_adam.evaluate(X_test, Y_test)\n",
    "print(f\"Test loss: {loss:.4f}\")\n",
    "print(f\"Test accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fKh_2zyNtNJ"
   },
   "source": [
    "## Opdracht 2. Tanh activation function\n",
    "\n",
    "We hebben geleerd om te werken met de sigmoid activation function. Er zijn echter verschillende andere activation functions. De [tanh](https://www.youtube.com/watch?v=Xvg00QnyaIY&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0) functie lijkt erg op de sigmoid maar het verschil is dat de afgeleide van de tanh stijler is en dat deze naar -1 gaat. Er zijn een heleboel redenen om te kiezen voor de sigmoid of tanh maar die zijn allemaal afhankelijk van het gegeven probleem. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOTFWwk8LY8D"
   },
   "source": [
    "### Opdracht\n",
    "Experimenteer met de tanh activation function en vergelijk de resultaten met de sigmoid function. Kijk naar de snelheid van trainen en de score van het model. Maak daartoe in onderstaande cel hetzelfde model, maar nu met de tanh-functie (zie ook https://www.tensorflow.org/api_docs/python/tf/keras/activations/tanh)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t3hd372N9Mu8",
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YS-4o0nENtNJ"
   },
   "source": [
    "## Opdracht 3. Learning rate decay\n",
    "Er is geen stelregel om de juiste learning rate te kiezen, je zult moeten experimenteren met verschillende waardes. Daarnaast blijk het in de praktijk niet handig te zijn om met een constante learning rate te werken. Als je langer traint, kom je dichter bij het optimum. Op dat moment is het handiger om kleine stapjes naar dat optimum te nemen. In de praktijk is het dus handig om de learning rate te laten afnemen. Dat wordt [learning rate decay](https://www.youtube.com/watch?v=QzulmoOg2JE&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc) genoemd.\n",
    "\n",
    "Dat kun je gepland doen met een learning rate scheduler. Na een bepaald aantal stappen of na een bepaalde tijd kun je de learning rate laten aanpassen. Een alternatief van learning rate scheduling zijn adaptive learning rate methodes: deze methodes maken de learning rate afhankelijk van de gevonden gradient. **Gebruik deze alleen wanneer je ze snapt, anders is een learning rate scheduler beter.**\n",
    "\n",
    "### Opdracht\n",
    "Bekijk de [documentatie](https://keras.io/api/optimizers/learning_rate_schedules/exponential_decay/) van Keras en voeg code voor de `ExponentialDecay` scheduler toe aan het beste model dat je tot nu toe gevonden hebt. Experimenteer met de initiële learning rate, de stappen en het verval. Vergelijk de resultaten met de vorige experimenten. Kijk naar de snelheid van trainen en de score van het model. Zie ook https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zKFmIUUAA81x",
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8F-KlRlNtNK"
   },
   "source": [
    "## Opdracht 4. L2 regularisatie\n",
    "[Regularization](http://neuralnetworksanddeeplearning.com/chap3.html#other_techniques_for_regularization) is net als dropout een manier om overfitting te voorkomen. Een ander gevolg van overfitten zijn hoge gewichten waardoor er dus ook weer een sterke afhankelijkheid is tussen input en output neuronen. Het idee van regularization is om hoge gewichten te voorkomen. \n",
    "\n",
    "Dat doe je door de cost functie uit te breiden met een tweede deel waarin je de gekwadrateerde gewichten bij elkaar optelt. Het netwerk wordt hierdoor gestraft wanneer gewichten heel hoog worden. L2 regularization is een veelgebruikte manier om dit te doen.\n",
    "\n",
    "### Opdracht\n",
    "Bekijk de [documentatie](https://keras.io/api/layers/regularizers/) van Keras en voeg code voor de `L2` regularizer toe aan het beste model dat je tot nu toe gevonden hebt. Vergelijk de resultaten met de vorige experimenten. Kijk naar de snelheid van trainen en de score van het model. Zie ook https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/L2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "COLo4CfcCf3D",
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nS_oRZ9-UzFi"
   },
   "source": [
    "## Opdracht 5. Alles op een hoop!\n",
    "Als het goed is, hebben de meeste experimenten voor betere resultaten gezorgd. Probeer nu om de methodes te combineren tot een überalwetend neural network! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zz8B-MVtUzFy",
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aF3uCdCjv1mu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "Les 9. Workshop neurale netwerken met TensorFlow en Keras",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}