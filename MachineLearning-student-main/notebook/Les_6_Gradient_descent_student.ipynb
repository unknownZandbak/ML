{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "\n",
    "In dit notebook staan een aantal opgaven die als doel hebben de werking van *gradient descent* te leren begrijpen. Het gradient descent algoritme heeft als doel het stapsgewijs vinden van een / de minimumwaarde van een functie. We zullen het algoritme handmatig toepassen en het algoritme implementeren in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Matplotlib version\", matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "docent"
    ]
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deel I. Simpele kwadratische functie\n",
    "\n",
    "Gegeven de functie:\n",
    "\n",
    "$$ f_1(x) = {x^2} $$\n",
    "\n",
    "Voor deze functie is het eenvoudig om de minimumwaarde te vinden: $x=0$ geeft het minimum $f_1(x) = 0$. Er zijn verschillende manieren om dat minimum te vinden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotten\n",
    "In onderstaande plot kun je in één oogopslag het minimum zien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functie voor f1(x)\n",
    "def f1(x):\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input en output f1(x) bepalen\n",
    "x = np.linspace(-5,5,1000) \n",
    "y = f1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input en output f1(x) plotten\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Afgeleide bepalen\n",
    "\n",
    "In een grafiek kun je niet altijd precies de waardes aflezen. Je kunt in dat geval de afgeleide gebruiken: daar waar de afgeleide 0 is, is er sprake van een _maximum_ (of 'piek') of een _minimum_ (een 'dal'). Dit hoeft niet het _globale_ maximum of minimum te zijn, maar kan ook een _lokaal_ maximum of minimum zijn. Als er sprake is van een functie in meerdere dimensies, kan een afgeleide van 0 ook betekenen dat je op een _zadelpunt_ zit (het maximum van de ene variabele en het minimum van de andere variabele).\n",
    "\n",
    "De afgeleide van $f_1(x)$ is simpel te bepalen:\n",
    "\n",
    "$$ f_1'(x) = {2x} $$\n",
    "\n",
    "Door vervolgens de vergelijking $f_1'(x) = 0$ op te lossen, vind je het (in dit geval) minimum:\n",
    "\n",
    "\\begin{align}\n",
    "    f_1'(x) &= 0  \\\\\n",
    "    2x &= 0       \\\\\n",
    "    x &= 0\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "Helaas is het leven niet altijd zo eenvoudig. Functies met meer dan drie variabelen kun je niet plotten en berekenen waar de afgeleide van een functie 0 is, is vaak niet haalbaar. Gelukkig hebben we het gradient descent algoritme. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oefening 6.1a\n",
    "\n",
    "Pas het gradient descent algoritme **handmatig** toe om het minimum van $f_1(x)$ te vinden voor een viertal scenario's:\n",
    "\n",
    "1. Startpunt $x = 1$ en learning rate $\\eta = 1.0$\n",
    "2. Startpunt $x = 1$ en learning rate $\\eta = 1.1$\n",
    "3. Startpunt $x = 1$ en learning rate $\\eta = 0.9$\n",
    "4. Startpunt $x = 1$ en learning rate $\\eta = 0.1$\n",
    "\n",
    "Ga als volgt te werk:\n",
    "- Bepaal de afgeleide $f'_1(x)$ van de functie $f_1(x)$\n",
    "- Bepaal de gradient bij $x = 1$\n",
    "- Bepaal met behulp van het startpunt, de gradient en de learning rate het vervolgpunt\n",
    "- Ga door tot dat de gradient kleiner dan 0.1 is of totdat je 5 iteraties hebt doorlopen\n",
    "- Plot het startpunt en de gevonden vervolgpunten op de grafiek van $f_1(x)$ om te visualiseren wat er gebeurt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student",
     "docent"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deel II. Lokaal en globaal minimum\n",
    "\n",
    "De tweede functie is wat complexer:\n",
    "\n",
    "\\begin{equation*}\n",
    "f_2(x) = {3^x} - {x^3}\n",
    "\\end{equation*}\n",
    "\n",
    "In onderstaande grafieken is te zien dat er twee minima zijn: een lokaal minimum en een globaal minimum. In de grafieken is niet te zien wat de exacte waarde voor `x` is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functie voor f2(x)\n",
    "def f2(x):\n",
    "    return 3**x - x**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input en output voor f2(x) bepalen\n",
    "x = np.linspace(-5,5,1000) \n",
    "y = f2(x)\n",
    "\n",
    "# Input en output plotten: complete grafiek\n",
    "plt.figure(figsize=(8,6), dpi=100)\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input en output plotten: inzoomen op minima\n",
    "plt.figure(figsize=(8,6), dpi=100)\n",
    "ax = plt.axes()\n",
    "ax.set(xlim=(-1,4), ylim=(-1, 4))\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oefening 6.1b\n",
    "\n",
    "Pas het gradient descent algoritme **handmatig** toe om *een* minimum van $f_2(x)$ te vinden voor een aantal scenario's:\n",
    "1. Startpunt $x = 4$ en learning rate $\\eta = 0.10$\n",
    "2. Startpunt $x = 4$ en learning rate $\\eta = 0.05$\n",
    "2. Startpunt $x = 4$ en learning rate $\\eta = 0.01$\n",
    "\n",
    "Ga als volgt te werk:\n",
    "- Bepaal de afgeleide $f'_2(x)$ van de functie $f_2(x)$\n",
    "- Bepaal de gradient voor $x = 4$\n",
    "- Bepaal met behulp van het startpunt, de gradient en de learning rate het vervolgpunt\n",
    "- Ga door tot dat de gradient kleiner dan 0.1 is of tot dat je 5 iteraties hebt doorlopen\n",
    "- Plot het startpunt en de gevonden vervolgpunten op de grafiek van $f_2(x)$ om te visualiseren wat er gebeurt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schrijf code voor het gradient descent algoritme. Pas de code toe op de hierboven genoemde scenario's.\n",
    "\n",
    "Ga als volgt te werk:\n",
    "- Schrijf een functie `df2(x)` voor $f'_2(x)$ die als input een waarde van `x` krijgt en als output de gradient op dat punt `x` geeft\n",
    "- Schrijf een functie `gd_f2(x, eta)` voor gradient descent voor $f_2(x)$ die als input een startpunt `x` en learning rate `eta` krijgt en als returnwaarde `(path_x, path_y)` de *paden* van zowel `x` als `y` geeft. De returnwaarde `path_x` ziet er bijvoorbeeld zo uit na 4 iteraties met startpunt `x = 4`: `path_x = [4, 3, 2, 1, 0]`. In `path_y` zitten dan de bijbehorende y-waarden met `y` = `f2(x)`.\n",
    "- Maar stel eerst **pseudocode** op!\n",
    "- Gebruik de returnwaarde `(path_x, path_y)` om het pad te visualiseren dat door gradient descent wordt afgelegd. Plot dit op de grafiek van $f_2(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deel III. Meerdere dimensies\n",
    "\n",
    "De derde functie bevat niet één, maar twee variabelen:\n",
    "\n",
    "\\begin{equation*}\n",
    "f_3(x_1, x_2) = {x_1^2} + {2 x_{2}^2}\n",
    "\\end{equation*}\n",
    "\n",
    "Het is simpel in te zien dat het minimum bij $f_3(0, 0)$ ligt. Het is hieronder ook te zien:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f3(x1, x2):\n",
    "    return x1**2 + 2*x2**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.linspace(-5,5,10) \n",
    "x2 = np.linspace(-5,5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.zeros((len(x1),len(x2)))\n",
    "for a in range(0,len(x1)):\n",
    "    for b in range(0,len(x2)):\n",
    "        y[a][b] = f3(x1[a], x2[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.contour3D(x1, x2, y, 50, cmap='binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelfde figuur, maar dan recht van boven\n",
    "ax.view_init(90, 0)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oefening 6.1c\n",
    "\n",
    "Pas het gradient descent algoritme **handmatig** toe om *een* minimum van $f_3(x_1, x_2) = {x_1^2} + {2 x_{2}^2}$ te vinden voor onderstaande scenario's:\n",
    "- Startpunt $(x_1, x_2) = (-4, 4)$, learning rate $\\eta = 0.4$\n",
    "- Startpunt $(x_1, x_2) = (-4, 4)$, learning rate $\\eta = 0.04$\n",
    "\n",
    "Ga als volgt te werk:\n",
    "- Bepaal de partieel afgeleides van $f_3(x_1, x_2)$ voor $x_1$ en $x_2$, dus:\n",
    "\n",
    "$$ \\frac{\\partial f_3}{\\partial x_1} \\text{ en } \\frac{\\partial f_3}{\\partial x_2} $$ \n",
    "\n",
    "- Bepaal de gradients voor zowel $x_1$ als $x_2$ voor het gegeven startpunt\n",
    "- Bepaal met behulp van het startpunt, de gradient en de learning rate het vervolgpunt\n",
    "- Ga door tot dat de gradient kleiner dan 0.1 is of tot dat je 5 iteraties hebt doorlopen\n",
    "\n",
    "NB. Let op het verschil tussen de stappen die gemaakt worden voor $x_1$ en $x_2$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deel IV. De eindbaas\n",
    "\n",
    "De *Bowser* van onze functies ziet er als volgt uit:\n",
    "\n",
    "\\begin{equation*}\n",
    "f_{4}(x_1, x_2) = {3^{x_1}} - {x_1^3} + {3^{x_2}} - {x_2^3}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f4(x1, x2):\n",
    "    return 3**x1 - x1**3 + 3**x2 - x2**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.linspace(-3,5,1000) \n",
    "x2 = np.linspace(-3,5,1000)\n",
    "\n",
    "y = np.zeros((len(x1),len(x2)))\n",
    "for a in range(0,len(x1)):\n",
    "    for b in range(0,len(x2)):\n",
    "        y[a][b] = f4(x1[a], x2[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.contour3D(x1, x2, y, 50, cmap='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De beperkingen van de 3D plots van Matplotlib komen hier aan het licht, want het is helaas niet zo duidelijk dat de bodem niet vlak is, maar glooiend. Er is dus sprake van meerdere (lokale én globale) minima. Hieronder een interactieve plot, waarbij je dit beter ziet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(data=[go.Surface(z=y)])\n",
    "fig.update_layout(title='Eindbaas', autosize=False, width=800, height=800, margin=dict(l=65, r=50, b=65, t=90))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "docent"
    ]
   },
   "outputs": [],
   "source": [
    "# # Importeer holoviews en plotly eerst\n",
    "# import holoviews as hv\n",
    "# hv.extension('plotly')\n",
    "# print(\"Holoviews version\", hv.__version__)\n",
    "\n",
    "# surface = hv.Surface(y, bounds=(-5, -5, 5, 5))\n",
    "# surface.opts(width=500, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oefening 6.1d\n",
    "  (optioneel)\n",
    "\n",
    "Pas het gradient descent algoritme **handmatig** toe om *een* minimum van $f_{4}(x_1, x_2) = {3^{x_1}} - {x_1^3} + {3^{x_2}} - {x_2^3}$ te vinden voor de volgende scenario's:\n",
    "\n",
    "- Startpunt $(x_1, x_2) = (2, 1)$, learning rate $\\eta = 0.5$\n",
    "- Startpunt $(x_1, x_2) = (2, 1)$, learning rate $\\eta = 0.05$\n",
    "\n",
    "Ga als volgt te werk:\n",
    "\n",
    "- Bepaal de afgeleides van $f_4(x_1, x_2)$\n",
    "- Bepaal de gradient op $(x_1, x_2) = (2, 1)$\n",
    "- Bepaal met behulp van het startpunt, de gradient en de learning rate het vervolgpunt\n",
    "- Ga door tot dat de gradient kleiner dan 0.1 is of tot dat je 5 iteraties hebt doorlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breid de code voor het gradient descent algoritme uit om te kunnen werken met twee inputvariabelen. Pas de code toe op de hierboven genoemde scenario's.\n",
    "\n",
    "Ga als volgt te werk:\n",
    "- Schrijf voor beide afgeleides van $f_4(x_1, x_2)$ een functie die respectievelijk als input een waarde `x1` of `x2` krijgt en als output `y` geeft\n",
    "- Schrijf een functie `gd_f4(x1, x2, eta)` voor gradient descent voor $f_4(x_1, x_2)$ die als input twee startpunten `x1` en `x2` en een learning rate `eta` krijgt en als returnwaarde `(path_x1, path_x2, path_y)` geeft. \n",
    "- Maar stel eerst **pseudocode** op! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}