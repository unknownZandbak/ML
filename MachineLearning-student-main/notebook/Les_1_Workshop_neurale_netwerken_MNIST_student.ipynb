{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zy0Y6xTLNtM6"
   },
   "source": [
    "# Introductie Neurale Netwerken met Keras en TensorFlow\n",
    "\n",
    "- Originele versie: Daniel Moser (UT Southwestern Medical Center)\n",
    "- Aangepast door Tijmen Muller (tijmen.muller@hu.nl) en Joost Vanstreels (joost.vanstreels@hu.nl) (2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cro1QI9NtM7"
   },
   "source": [
    "Tijdens de cursus Machine Learning gaan we aan de slag met neurale netwerken en convolutionele neurale netwerken (ConvNets), ook wel bekend als deep learning. Tijdens deze workshop gaan we de basisstappen doorlopen om een simpel neuraal netwerk te maken met behulp van de populaire libraries van Google, Keras en TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgLiUhUr4t4d"
   },
   "source": [
    "## Inleiding\n",
    "\n",
    "### Doel workshop\n",
    "\n",
    "Het doel van deze workshop is om een beeld te krijgen van de werking van neurale netwerken:\n",
    "- Wat zijn neurale netwerken?\n",
    "- Hoe werken ze?\n",
    "- Voor wat voor een type problemen kun je ze inzetten?\n",
    "- Wat is het verschil met machine learning technieken zoals decision trees en kNN?\n",
    "- Wat is de toegevoegde waarde van convolutionele netwerken?\n",
    "\n",
    "### Opzet workshop\n",
    "\n",
    "Aan de hand van theoretische uitleg en praktische codevoorbeelden gaan jullie zelfstandig aan de slag met enkele opdrachten:\n",
    "1. We starten met het opzetten van een simpel neuraal netwerk dat jullie gaan *tweaken* om betere resultaten te krijgen;\n",
    "2. Daarna gaan jullie het neuraal netwerk uitbreiden met meerdere lagen;\n",
    "3. Ook dit model gaan jullie tweaken op zoek naar 98% accuracy;\n",
    "4. Tenslotte gaan we hetzelfde proberen te realiseren met kNN.\n",
    "\n",
    "### Werkwijze\n",
    "De werkwijze is als volgt:\n",
    "- Lees de uitleg goed;\n",
    "- Voer de code stap-voor-stap uit;\n",
    "- Voer de opdrachten uit -- de cellen die getagd zijn met `student` moet je zelf invullen;\n",
    "- Voor het uitvoeren van de opdrachten heb je geen externe bronnen nodig: alle benodigde kennis is gegeven in dit notebook;\n",
    "- Installeer zelf de benodigde libraries (`keras` en `tensorflow`) en draai het notebook bij voorkeur lokaal. Mocht je het niet werkend krijgen, dan kun je uitwijken naar Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpfctwLyNtM7"
   },
   "source": [
    "## Deel I. Machine learning met neurale netwerken\n",
    "\n",
    "### Herkennen van handgeschreven cijfers\n",
    "\n",
    "We gaan aan de slag met de '*Hello World*' van neurale netwerken: het herkennen van de handgeschreven cijfers van de [MNIST dataset](http://yann.lecun.com/exdb/mnist/). Deze dataset bestaat uit een trainset van 60.000 plaatjes met cijfers en een testset van nog eens 10.000 plaatjes.\n",
    "\n",
    "De plaatjes zien er als volgt uit:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mo8rdjbBNtM7"
   },
   "source": [
    "<a title=\"Josef Steppan, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:MnistExamples.png\"><img width=\"512\" alt=\"MnistExamples\" src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OjDwn6ArNtM7"
   },
   "source": [
    "### Benodigde libraries \n",
    "\n",
    "We hebben een aantal libraries nodig, sommigen kennen jullie al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QfscTgLkNtM8"
   },
   "outputs": [],
   "source": [
    "import random  \n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Keras/Tensorflow version\", tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QljqpqwxNtM8"
   },
   "source": [
    "### Stap 1. Data exploration \n",
    "\n",
    "De MNIST dataset is gebundeld in Keras, we kunnen deze eenvoudig importeren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LRVKdbbDNtM8",
    "outputId": "5a2cb07f-c659-475e-8f5f-c32672fd73d8"
   },
   "outputs": [],
   "source": [
    "# Inladen van de dataset, deze is al gesplitst in een train- en testset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print(\"x_train shape\", x_train.shape)\n",
    "print(\"y_train shape\", y_train.shape)\n",
    "print(\"x_test shape\", x_test.shape)\n",
    "print(\"y_test shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bybxKdkLOZFd"
   },
   "source": [
    "**Let op**.\n",
    "Als je kijkt naar de dataset dan zie je een groot verschil met de datasets die we gezien hebben bij Computational Modelling. Daar hadden we te maken met `.csv`-bestanden met datasets met meerdere kolommen en verschillende datatypes als inhoud. We konden kiezen welke features we als input gingen gebruiken. Met andere woorden: de inhoud van deze datasets was gestructureerd (tabulair). De MNIST dataset bestaat eigenlijk uit 60.000 keer 784 (28 x 28) pixelwaardes... dat is ongestructureerde data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_qbg_vsNtM9"
   },
   "source": [
    "Met matplotlib kunnen we een aantal plaatjes uit de dataset bekijken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 657
    },
    "id": "ivZmzp_MNtM9",
    "outputId": "bdd324cc-ddb1-4f79-815a-3c8b1ecb50f2"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    mnist_number = random.randint(0, len(x_train))\n",
    "    plt.imshow(x_train[mnist_number], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Class {}\".format(y_train[mnist_number]))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ieu_YyFSNtM9"
   },
   "source": [
    "Een plaatje is eigenlijk niks meer dan een twee dimensionaal array met 28 x 28 grijswaardes. Elke pixel is een 8-bit integer met waardes tussen 0 (zwart) en 255 (wit). Hieronder wordt de array getoond van het getal rechtsonder in bovenstaande figuur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pVUbivD8NtM9",
    "outputId": "762f7395-f951-49df-e35a-a1345f65c04d"
   },
   "outputs": [],
   "source": [
    "# just a little function for pretty printing a matrix\n",
    "def matprint(mat, fmt=\"g\"):\n",
    "    col_maxes = [max([len((\"{:\"+fmt+\"}\").format(x)) for x in col]) for col in mat.T]\n",
    "    for x in mat:\n",
    "        for i, y in enumerate(x):\n",
    "            print((\"{:\"+str(col_maxes[i])+fmt+\"}\").format(y), end=\"  \")\n",
    "        print(\"\")\n",
    "\n",
    "# now print!        \n",
    "matprint(x_train[mnist_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5E9Y2SMNtM-"
   },
   "source": [
    "### Stap 2. Data preparation\n",
    "\n",
    "#### Features\n",
    "\n",
    "De input voor het neuraal netwerk is het plaatje. Dat plaatje bestaat eigenlijk uit 784 features: alle 784 pixels. Zo'n 28 x 28 matrix is niet handig als input voor een neuraal netwerk. We moeten er een vector van maken door _flattening_ toe te passen (zie figuur hieronder). We krijgen dan een vector met een lengte van 784 (= 28 x 28).\n",
    "\n",
    "Daarnaast is het handig om de data te normaliseren. De Z-score gebruiken is hier niet logisch, dus we gaan indexeren waarbij 0 (het minimum) de waarde 0 wordt, 255 (het maximum) wordt 1 en alle tussenliggende waardes worden lineair gemapt naar een waarde tussen 0 en 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmvZVFR2NtM-"
   },
   "source": [
    "<img src='https://i.imgur.com/l049B93.png' >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8PWzi6ilNtM-",
    "outputId": "5213e0a0-ea89-4c87-8a5e-f49c87f85ee5"
   },
   "outputs": [],
   "source": [
    "# Let op: we schakelen hier over van een kleine letter naar een hoofdletter, zodat de originele\n",
    "# trainingsdata in `x_train` beschikbaar blijft!\n",
    "\n",
    "X_train = x_train.reshape(60000, 784) # reshape de 60.000 plaatjes van 28 x 28 matrices naar 60.000 784-lengte vectoren.\n",
    "X_test = x_test.reshape(10000, 784)\n",
    "\n",
    "X_train = X_train.astype('float32')   # verander datatype van integers naar 32-bit floats\n",
    "X_test = X_test.astype('float32') \n",
    "\n",
    "X_train /= 255                        # normaliseer de pixels door de waarde te delen door de maximale waarde (= 255)\n",
    "X_test /= 255\n",
    "\n",
    "print(\"Training matrix shape\", X_train.shape)\n",
    "print(\"Testing matrix shape\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBfvAPyKNtM-"
   },
   "source": [
    "#### Target\n",
    "\n",
    "De target variabele is een 0, 1, 2, ... of 9. Dat zijn de verschillende klasses of categorieÃ«n. Kijk bijvoorbeeld maar eens naar de inhoud van een willekeurig item uit y_train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rD1M2Ehy4t4k",
    "outputId": "03ed6a0e-529b-4803-8505-deb26c1bfd6b"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(X_train[mnist_number].reshape(28,28), cmap='gray', interpolation='none')\n",
    "print(\"Dit is een:\", y_train[mnist_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2WphOHCPWrI"
   },
   "source": [
    "Bij CM hebben we altijd gewerkt met Ã©Ã©n targetvariabele die meerdere waardes kon krijgen. Denk aan classificeren waarbij je verschillende klasses kon voorspellen of regressie waarbij je meerdere waardes kon voorspellen.\n",
    "\n",
    "In dit geval werkt het iets anders: we hebben meerdere targetvariabelen die allemaal corresponderen met Ã©Ã©n klasse. In dit geval hebben we 10 target variabelen die corresponderen met 0, 1, 2, ... of 9. \n",
    "\n",
    "We moeten de target aanpassen naar het volgende formaat:\n",
    "\n",
    "```\n",
    "0 -> [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "1 -> [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "2 -> [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "etc.\n",
    "```\n",
    "\n",
    "Het neuraal netwerk gaat straks proberen te voorspellen welk cijfer in een plaatje staat. Net zoals bij decision trees, kNN, etc. zal een voorspelling niet altijd 100% zeker zijn. Het zal vaak een gok zijn met een bepaalde betrouwbaarheid, bijvoorbeeld iets als dit:\n",
    "\n",
    "```\n",
    "[0.94, 0, 0, 0, 0, 0, 0.06, 0, .07, 0.11]\n",
    "```\n",
    "In dit geval is de voorspelling een '0' met een hoge zekerheid, maar geeft het neurale netwerk ook een kleine kans aan een '6', een '8' en een '9'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qbgh99vpNtM_"
   },
   "outputs": [],
   "source": [
    "## One-hot encoding\n",
    "nb_classes = 10 # aantal klassen\n",
    "\n",
    "Y_train = tf.keras.utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = tf.keras.utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "print(\"De\", y_train[mnist_number], \"is na one-hot encoding:\", Y_train[mnist_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2z7yyUUpNtM_"
   },
   "source": [
    "### Stap 3. Modelling\n",
    "#### Een eerste supersimpel netwerk\n",
    "We gaan toewerken naar een fully connected 3-layer network zoals hieronder. Merk op dat je het netwerk van onder naar boven moet lezen: de input (het plaatje van 28 bij 28) staat onderaan, de output (met andere woorden, de voorspelde categorie) staat bovenaan.\n",
    "\n",
    "<img src=\"https://i.imgur.com/1MR9U5c.png\" />\n",
    "\n",
    "Maar voordat we dat gaan doen, gaan we eerst een heel simpel netwerk maken dat maar uit Ã©Ã©n laag bestaat. De input voor het netwerk zijn de 784 pixels en de output zijn de 10 mogelijke klasses:\n",
    "\n",
    "<img src=\"https://i.imgur.com/QYNwWcB.png\" />\n",
    "\n",
    "#### Hoe werkt een neuraal netwerk?\n",
    "\n",
    "Een neuraal netwerk is eigenlijk niks meer dan een heleboel berekeningen. Een netwerk bestaat uit *neuronen* en *verbindingen*. Voor alle 784 pixels zijn neuronen gemaakt die zijn gekoppeld aan alle 10 outputklasses, ook hiervoor zijn neuronen gemaakt. Er zijn in totaal 7.840 verbindingen tussen alle neuronen van de inputlaag en de neuronen van de outputlaag. \n",
    "\n",
    "Elke verbinding heeft een gewicht. De waarde van elke pixel is de waarde van de inputneuron, deze wordt vermenigvuldigd met het gewicht van een verbinding. Elk van de 10 outputneuronen heeft als input de som van alle vermenigvuldigingen. De klasse van de outputneuron waarvan de som het hoogste is, is de voorspelling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LIqyfoI4t4l"
   },
   "source": [
    "#### Model opbouwen\n",
    "Net zoals bij CM, moet je eerst een model initialiseren. Bij het inladen van de libraries hebben we het `Sequential` model ingeladen. Het sequential model is een lineaire *opstapeling* van lagen en wordt veel gebruikt in de praktijk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R51KiCCuNtM_"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQDotXbcNtM_"
   },
   "source": [
    "Daarna moeten we handmatig de lagen van het model initialiseren. In dit geval is er maar Ã©Ã©n laag. Deze code voegt een laag toe aan het model:\n",
    "\n",
    "```\n",
    "model.add(Dense(y, input_shape=(x,)))\n",
    "```\n",
    "\n",
    "Voor deze laag moet je specificeren hoeveel inputneuronen (`x`) en outputneuronen (`y`) er zijn op basis van de instructies hierboven. Merk op: `(x,)` is geen typo, dat staat voor een vector met lengte `x`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "XzRjPOZ3NtNA",
    "outputId": "2eb33154-5b7b-4b1d-f366-5b57b460d437",
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9R8sKxKh4t4m"
   },
   "source": [
    "Neurale netwerken kennen *activatiefuncties*. Dat zijn berekeningen om iets slims te doen met de som van alle vermenigvuldigingen die als input binnen komen. Later zullen we zien waarom dat zo belangrijk is. In dit geval nemen we de _sigmoid function_ als activatiefunctie. Deze zorgt ervoor dat de output nooit lager dan -1 of hoger dan 1 wordt. Dit is een soort van normaliseren. Voor de volledigheid, de formule van de sigmoid is:\n",
    "\n",
    "$$ a(z) = \\frac{1}{1 + {\\rm e}^{-z}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvGFbxB5NtNA"
   },
   "source": [
    "<a title=\"Qef, Public domain, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Logistic-curve.svg\"><img width=\"512\" alt=\"Logistic-curve\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/512px-Logistic-curve.svg.png\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-L3lz7mCNtNA"
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9mdtE7U4t4n"
   },
   "source": [
    "Dropout zet telkens verschillende willekeurige nodes van de laag op nul. Dit wordt gedaan om _overfitting_ tegen te gaan. Je mag dit weer vergeten op dit moment -- we komen er later in het vak op terug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oG7UgpqmNtNB"
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhVRDLgO4t4o"
   },
   "source": [
    "Nu hebben we ons model helemaal gebouwd. Hieronder zie je als het goed is de onderstaande samenvatting het eindresultaat:\n",
    "\n",
    "```\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense (Dense)                (None, 10)                7850      \n",
    "_________________________________________________________________\n",
    "activation (Activation)      (None, 10)                0         \n",
    "_________________________________________________________________\n",
    "dropout (Dropout)            (None, 10)                0         \n",
    "=================================================================\n",
    "Total params: 7,850\n",
    "Trainable params: 7,850\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5F5t-aCo4t4o"
   },
   "outputs": [],
   "source": [
    "model.summary()\n",
    "\n",
    "# We slaan de parameters van het model op, zodat je ze eventueel\n",
    "# later kunt gebruiken\n",
    "weights_init = model.get_weights()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udLdi_-QNtND"
   },
   "source": [
    "#### Het model *compilen*\n",
    "\n",
    "Bij CM hebben we gezien dat een model allerlei hyperparameters kent zoals de diepte van een beslisboom, de afstandsfunctie bij _k_-NN of het aantal clusters bij kMeans. Er zijn ook hyperparameters voor het kiezen van een bepaald optimalisatiealgoritme. De belangrijkste reden is dat het optimalisatiealgoritme niet zo belangrijk is bij de modellen die we bij CM gebruikt hebben. Het trainen (of fitten) van het model met het default algoritme lukte altijd goed en snel.\n",
    "\n",
    "Bij neurale netwerken werkt het trainen van model anders. Het is een stuk complexer waardoor er twee risico's zijn:\n",
    "- Je vindt geen goede oplossing \n",
    "- Het vinden van een goede oplossing duurt lang\n",
    "\n",
    "Daarom moet je bij een neuraal netwerk goed nadenken over de **loss function** en de **optimizer**. Bij deze cursus zullen we kijken naar trainen met behulp van **gradient descent**. Daarbij is de **learning rate** erg belangrijk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpESZz2yNtNE"
   },
   "source": [
    "Gradient descent is het zoeken naar een minimum:\n",
    "<img src = \"https://i.imgur.com/weH0O4U.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIAUaH6fNtNE"
   },
   "source": [
    "Je kunt dat minimum niet zien, dus je moet stapjes in de juiste richting nemen. De grootte van een stap is de learning rate. Kleine stapjes nemen duurt lang, maar te grote stappen is ook niet altijd goed:\n",
    "<img src = \"https://i.imgur.com/jq6rFFa.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slzC1rVENtNF"
   },
   "source": [
    "Kleine stapjes zijn niet altijd beter. Het risico bij kleine stapjes is dat je blijft hangen bij een lokaal minimum, terwijl er andere (lagere) minima bestaan. Het vinden van het globale minimum is de optimale situatie.\n",
    "\n",
    "<img src = 'https://i.imgur.com/fft8oPH.png' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_R1zWt24t4p"
   },
   "source": [
    "Enfin, hier gaan we later mee aan de slag. Voor nu kiezen we voor de volgende setup van het model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZ2M6mNoNtNF"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U72jBYYo4t4q"
   },
   "source": [
    "#### Het model trainen\n",
    "\n",
    "Nu kunnen we eindelijk het model gaan trainen! Zoals gezegd, werkt dat anders dan bij CM. Alle verbindingen moeten berekend worden. Dat doe je stap-voor-stap: \n",
    "- Gegeven elk item uit de traindata\n",
    "- Bepaal de output van het neuraal netwerk\n",
    "- Vergelijk de output met de echte waarde\n",
    "- Als de waarde goed is, ga je naar het volgende item\n",
    "- Als de waarde niet goed is, dan pas je de gewichten van de verbindingen aan die tot het verkeerde antwoord geleid hebben\n",
    "- Enzovoorts\n",
    "\n",
    "Dat zijn heel veel berekeningen... en dat kost ook heel veel tijd... daarom ga je bij neurale netwerken niet op zoek naar de optimale oplossing, maar naar een oplossing die goed genoeg is. Of naar de beste oplossing gegeven een bepaalde tijd (_runtime_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EAwHn8YNtNG"
   },
   "source": [
    "Een manier om te spelen met het trainen, is de keuze voor een **batch size**. Dit bepaalt hoeveel items uit de dataset per stap bekeken worden, voordat de gewichten geÃ¼pdate worden. Grotere batch sizes zorgt voor nauwkeuriger trainen, de updates van de parameters zijn dan erg precies. Een kleine batch size (waarbij _online learning_ met batch size gelijk aan 1 de kleinste optie is) zorgt voor snelle updates (na elke batch een update), maar is minder nauwkeurig qua richting. Ook complexiteit speelt een rol bij het kiezen van batch size: een grotere batch gebruikt meer geheugen, maar is computationeel sneller. In de praktijk blijkt een grote batch size de neiging tot overfitting te hebben, maar de oorzaak hiervan is niet helemaal duidelijk. Het zoeken naar de beste batch size is vaak een kwestie van trial en error. Een goed startpunt is een batch size tussen de 32 en de 512 datapunten.\n",
    "\n",
    "Daarnaast is het aantal **epochs** ook belangrijk. Dit geeft aan hoe vaak je de hele training set wilt gebruiken. Het is niet zo dat je na het bekijken van alle items uit de trainset klaar bent, je kunt de trainset nog een keer gebruiken om het model te verbeteren. En nog een keer. Enzovoorts. Zie het als het studeren voor een tentamen waarbij je een oefententamen vaker maakte om de stof te begrijpen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uza088eBNtNH"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "          batch_size=512, epochs=1,\n",
    "          verbose=1) # verbose zorgt voor een animatie van de voortgang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oj3-sTMGNtNH"
   },
   "source": [
    "Je ziet de **loss** en **accuracy**. De accuracy kennen jullie: dat het percentage goed voorspelde cijfers. De loss is het resultaat van de _loss function_ (ook wel _cost function_ genaamd). Het is nog te vroeg om hier dieper op in te gaan, maar onthoud voor nu dat het een foutmaat is, dus we willen hier een zo klein mogelijk getal zien (in de perfecte wereld is de loss gelijk aan 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JT4Cx2u7NtNH"
   },
   "source": [
    "### Stap 4. Evalueren\n",
    "\n",
    "Leuk, die accuracy op de traindata, maar hoe goed doet ie het op de testdata?! We kijken weer naar de accuracy maar ook naar de score (alhoewel die minder interessant is op dit moment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8entMBoNtNH"
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGUy0tt6NtNI"
   },
   "source": [
    "De algemene evaluatie is belangrijk, maar het is ook goed om specifieke / individuele voorspellingen te bekijken. Hieronder zie je een visualisatie van correct en incorrect voorspelde cijfers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vf6QQ394NtNI"
   },
   "outputs": [],
   "source": [
    "# The predict_classes function outputs the highest probability class\n",
    "# according to the trained classifier for each input example.\n",
    "# predicted_classes = model.predict_classes(X_test)  # deprecated\n",
    "predicted_classes = np.argmax(model.predict(X_test), axis=-1)\n",
    "\n",
    "# Check which items we got right / wrong\n",
    "correct_indices = np.nonzero(predicted_classes == y_test)[0]\n",
    "incorrect_indices = np.nonzero(predicted_classes != y_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_DV1atVNtNJ"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "for i, correct in enumerate(correct_indices[:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_test[correct].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[correct], y_test[correct]))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "for i, incorrect in enumerate(incorrect_indices[:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[incorrect], y_test[incorrect]))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpuNWyPp4t4s"
   },
   "source": [
    "### Opdracht 1. De accuracy verhogen\n",
    "\n",
    "Ga een paar stappen terug en kijk of je de accuracy kunt verhogen. Dat kan op verschillende manieren.\n",
    "Kijk ook naar de benodigde tijd: duurt het trainen langer of korter?\n",
    "\n",
    "Speel met:\n",
    "- epochs\n",
    "- batch size\n",
    "\n",
    "**Merk op**: wanneer je meerdere keren achter elkaar `model.fit` aanroept, train je het model steeds verder. Het is dus handig om na elk experiment opnieuw het model op te bouwen om vanaf *scratch* te beginnen. Je kunt ook gebruik maken van de eerder opgeslagen (random) gewichten in `weights_init`, als je dat handig vindt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZ8--sKH4t4t"
   },
   "source": [
    "Geef hieronder aan welke accuracy je hebt behaald bij welk aantal epochs en welke batch size. Als je in de buurt bent van 92% accuracy op de testset, dan kun je door naar de volgende opdracht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "source": [
    "_Geef hier het antwoord._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1UNAal64t4t"
   },
   "source": [
    "### Opdracht 2. Modelling van een completer netwerk\n",
    "#### Een fully connected 3-layer network\n",
    "We gaan ons supersimpele netwerk nu uitbouwen tot een fully connected 3-layer network zoals hieronder:\n",
    "\n",
    "<img src=\"https://i.imgur.com/1MR9U5c.png\" />\n",
    "\n",
    "#### Waarom meer lagen?!\n",
    "\n",
    "Het idee is dat een netwerk met meerdere lagen, complexere taken kan uitvoeren. In theorie zou de accuracy voor de MNIST dataset hiermee verhoogd moeten kunnen worden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hJk5zmxzaQS"
   },
   "source": [
    "### Opdracht 2.a. Het netwerk opbouwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kXBwBrMk4t4t"
   },
   "outputs": [],
   "source": [
    "# We starten weer met het initialiseren van het model\n",
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCHkvLi74t4t"
   },
   "source": [
    "#### De eerste laag\n",
    "\n",
    "In ons eerste model hadden we een input van 784 en output van 10. Nu is de input weer 784 maar de output is een eerste **hidden layer** van 512 neuronen. Voer de volgende zaken uit in cellen hieronder:\n",
    "\n",
    "1. Gebruik `model.add()` om deze laag toe te voegen aan het model\n",
    "2. Gebruik de sigmoid als activatiefunctie\n",
    "3. Voeg de dropout zeroes toe met waarde 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RslIFi8O4t4u",
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MbeSEt1d4t4u",
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npICIBEL4t4v",
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tclj9vPINtNC"
   },
   "source": [
    "#### De tweede laag\n",
    "\n",
    "Bij de opvolgende lagen hoef je geen input te definiÃ«ren: de vorige laag dient automatisch als input.\n",
    "\n",
    "- Voeg hieronder nog een laag van 512 neuronen toe aan het model met de sigmoid activatiefunctie en een dropout van 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwuGgKhPNtNC",
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSDltoq3NtNC"
   },
   "source": [
    "####  De output laag\n",
    "\n",
    "De laatste laag heeft 10 elementen, namelijk de 10 klasses. \n",
    "\n",
    "- Voeg een laag van 10 neuronen toe aan het model met de activatiefunctie 'softmax'; een dropout is niet nodig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2rjYaltNtND",
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RUx-anMmNtND"
   },
   "outputs": [],
   "source": [
    "# Laten we even kijken hoe dat eruit ziet\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCLXBjRwzaQW"
   },
   "source": [
    "Als het goed is, ziet de samenvatting van het model hierboven er als volgt uit:\n",
    "\n",
    "```\n",
    "Model: \"sequential_1\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_1 (Dense)              (None, 512)               401920    \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 512)               0         \n",
    "_________________________________________________________________\n",
    "dropout_1 (Dropout)          (None, 512)               0         \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 512)               262656    \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 512)               0         \n",
    "_________________________________________________________________\n",
    "dropout_2 (Dropout)          (None, 512)               0         \n",
    "_________________________________________________________________\n",
    "dense_3 (Dense)              (None, 10)                5130      \n",
    "_________________________________________________________________\n",
    "activation_3 (Activation)    (None, 10)                0         \n",
    "=================================================================\n",
    "Total params: 669,706\n",
    "Trainable params: 669,706\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```\n",
    "\n",
    "NB. Met deze twee extra lagen is het totaal aantal te trainen parameters dus bijna een factor 100 gegroeid: 7850 naar __669706__!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZG6N2xS4t4w"
   },
   "source": [
    "#### Compilen\n",
    "\n",
    "Gebruik dezelfde instellingen voor het compileren van het model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CAcJ8UmP4t4w"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LqzDTSS4t4x"
   },
   "source": [
    "#### Trainen\n",
    "\n",
    "Gebruik dezelfde instellingen voor het trainen van het model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-JLEQ4204t4x"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "          batch_size=512, epochs=1,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7IEBc20n4t4x"
   },
   "source": [
    "#### Evalueren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QyGQxIps4t4x"
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fKh_2zyNtNJ"
   },
   "source": [
    "### Opdracht 2.b.\n",
    "\n",
    "Hoe goed heeft dit *uitgebreidere* netwerk het gedaan ten opzicht van ons 'supersimpele' netwerk? Als het goed is, al veel beter. Ga ook voor dit netwerk spelen met de epochs en batch size en kijk of je de accuracy kunt verhogen. Kijk ook weer naar de benodigde tijd: duurt het trainen langer of korter?\n",
    "\n",
    "Speel ook met het aantal neuronen in de hidden layers, wat is het effect als je bijvoorbeeld maar 10 neuronen gebruikt in plaats van 512? Voeg eventueel ook hidden layers toe.\n",
    "\n",
    "**Merk op:** vergeet niet om bij elk experiment het model opnieuw op te bouwen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrKrEz-64t4y"
   },
   "source": [
    "Geef hieronder aan welke accuracy je hebt behaald bij welk aantal epochs en welke batch size. Geef ook de structuur van je netwerk aan, dus het aantal layers en het aantal neuronen per layer.  Als je in de buurt bent van 95% accuracy op de testset, dan kun je door naar de volgende opdracht. Als het goed is, kun je nu tot zo'n 98% komen. Als dat gelukt is, kun je verder gaan met de volgende opdracht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "source": [
    "_Geef hier het antwoord._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YS-4o0nENtNJ"
   },
   "source": [
    "## Deel II. Blik op de toekomst: deep learning met convolutionele neurale netwerken\n",
    "\n",
    "### Beperkingen van neurale netwerken\n",
    "\n",
    "De gevonden resultaten zijn bemoedigend, maar neurale netwerken hebben hun beperkingen. Bekijk onderstaande varianties van een *3* uit de dataset:\n",
    "\n",
    "<img src=\"https://i.imgur.com/KfyF1v2.png\" />\n",
    "\n",
    "Het is telkens dezelfde 3 maar dan:\n",
    "- Verplaatst;\n",
    "- Verkleurd en ruis op de achtergrond;\n",
    "- Gedraaid;\n",
    "- Verkleind;\n",
    "- Met negatieve kleuren.\n",
    "\n",
    "In alle gevallen zal ons neuraal netwerk de mist in gaan omdat de pixels van deze drieÃ«n afwijken van de pixels van de drieÃ«n in de trainset. Je zou dit soort afwijkende drieÃ«n kunnen toevoegen aan de trainset maar bij een nieuwe afwijking slaat je netwerk weer eerst de plank mis. \n",
    "\n",
    "### Oplossing: kijken zoals mensen kijken\n",
    "\n",
    "Wanneer wij een plaatje zien, gaan wij niet naar alle pixels kijken en daarmee bepalen of een plaatje een 3 is of iets anders. Wij kijken naar bepaalde patronen, vormen en/of structuren, bijvoorbeeld de drie *pootjes* die een drie heeft of de twee *rondjes* die een acht vormen. We kijken naar scheidingen tussen objecten op een foto of zaken die op de voorgrond of achtergrond staan. Op die manier maken wij onze *voorspellingen*. En dat is precies wat convolutionele neurale netwerken doen.\n",
    "\n",
    "Voor deze workshop gaat het te ver om ConvNets te behandelen, we bewaren deze materie voor de laatste weken van het vak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ugdPPfrNtNO"
   },
   "source": [
    "## Deel III. Terug naar het verleden: MNIST met _k_-Nearest Neighbours\n",
    "\n",
    "Zou het ook mogelijk zijn om cijfers te herkennen met de modellen die we geleerd hebben bij CM? Het is een classificatieprobleem dus regressie en clustering vallen af. Decision Trees zijn niet handig bij grote hoeveelheden features maar kNN kan prima over weg met veel features en in theorie zou je kNN kunnen gebruiken voor de MNIST dataset. Maar werkt het ook in de praktijk?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxOG5FuHzaQY"
   },
   "source": [
    "### Opdracht 3.a. _k_-NN voor MNIST\n",
    "\n",
    "Duik het verleden in en trek _k_-NN van de plank om een classifier te maken voor de MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HzegdlJ6zaQZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split   \n",
    "from sklearn.neighbors import KNeighborsClassifier    # kNN from sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsiyaKBCzaQZ"
   },
   "source": [
    "Voer de volgende acties uit in de cellen hieronder:\n",
    "\n",
    "- Gebruik `X_train` en `y_train` om een kNN classifier te fitten. \n",
    "- Leg een voorspelling voor `X_test` vast in `y_pred`. \n",
    "- Vergelijk deze met `y_test` en bereken de accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07QJ0_otzaQZ",
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhHN4j-NzaQZ"
   },
   "source": [
    "### Resultaten analyseren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maak een _confusion matrix_ `cm` door de voorspelde klassen te vergelijken met de werkelijke klassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De cel hieronder visualiseert de confusion matrix `cm` voor meer inzicht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yilbvHsSzaQa"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 5), dpi=100)\n",
    "\n",
    "ax = sns.heatmap(pd.DataFrame(cm), annot=True, cmap='Greens', fmt='d')\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "\n",
    "ax.set_xlabel('voorspelde waarde')\n",
    "ax.set_ylabel('echte waarde')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrWemgjPzaQa"
   },
   "source": [
    "### Opdracht 3.b. Accuracy verhogen\n",
    "\n",
    "Speel met de hyperparameters van kNN om de accuracy te verhogen.\n",
    "Wat valt je op aan de accuracy van kNN t.o.v. het neuraal netwerk? En de performance van kNN t.o.v. het neuraal netwerk?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jl3eL1knzaQa"
   },
   "source": [
    "Geef hieronder de gevonden resultaten aan. Geef in ieder geval aan hoeveel _neighbors_ je hebt gebruikt, wat de afstandsfunctie was en hoe groot je train en je testset waren. Maak ook een vergelijking in performance tussen het neurale netwerk en _k_-NN.\n",
    "\n",
    "Als het goed is, kun je nu tot zo'n 97% komen. Als dat gelukt is, ben je klaar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "source": [
    "_Geef hier het antwoord._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zasdy-jIzaQa"
   },
   "source": [
    "### Beperkingen van kNN\n",
    "\n",
    "kNN doet het harstikke goed voor MNIST. Maar we hebben het hier over plaatjes van 28 x 28 pixels die eigenlijk zwart wit zijn. Wat als we complexere plaatjes zouden gebruiken? Of plaatjes van 1920x1080 pixels? De performance van kNN wordt dan dramatisch en de accuracy is dan ook bedroevend...\n",
    "\n",
    "#### Fashion MNIST\n",
    "De Fashion MNIST dataset lijkt op MNIST, maar de plaatjes zijn iets complexer. Kledingstukken lijken qua vorm meer op elkaar en er wordt gebruik gemaakt van meerdere grijstinten. \n",
    "\n",
    "<img src=\"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/02/Plot-of-a-Subset-of-Images-from-the-Fashion-MNIST-Dataset-1024x768.png\" />\n",
    "\n",
    "De accuracy van kNN duikt voor deze dataset al richting de 85%... niet best dus... en we hebben het hier nog steeds over superkleine plaatjes. Moet je na gaan wat er gebeurt bij grotere plaatjes.\n",
    "\n",
    "Enfin, als je het leuk vindt om met de Fashion MNIST dataset aan de slag te gaan, gebruik dan de volgende code: \n",
    "- `fashion_mnist = tf.keras.datasets.fashion_mnist`\n",
    "- `(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzeldshpzaQa"
   },
   "source": [
    "## Conclusies\n",
    "\n",
    "Hopelijk heb je door deze workshop een beter beeld van wat neurale netwerken zijn en hoe ze werken en hebben we jullie gemotiveerd voor de cursus Machine Learning!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "name": "Les 1. Workshop neurale netwerken MNIST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}